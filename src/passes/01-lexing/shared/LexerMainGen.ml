(* Vendor dependencies *)

module Region  = Simple_utils.Region
module Std     = Simple_utils.Std
module Config  = Preprocessor.Config
module Options = LexerLib.Options
module Client  = LexerLib.Client
module Unit    = LexerLib.Unit

(* CLI errors *)

let red msg = Printf.sprintf "\027[31m%s\027[0m" msg

(* The functor *)

module Make (Config  : Config.S)
            (Options : Options.S)
            (Token   : Token.S)
            (Passes  : Pipeline.PASSES
                       with type lex_unit = Token.t Unit.t) =
  struct
    (* Reading the preprocessor CLI *)

    module PreprocParams = Preprocessor.CLI.Make (Config)

    (* Reading the lexer CLI *)

    module Parameters = LexerLib.CLI.Make (PreprocParams)

    (* Checking for errors and valid exits *)

    type cli_status =
      Ok
    | Info  of string
    | Error of string

    let check_cli () : cli_status =
      match Parameters.Status.status with
        `SyntaxError  msg
      | `WrongFileExt msg
      | `FileNotFound msg -> Error (red msg)
      | `Help         buf
      | `CLI          buf -> Info (Buffer.contents buf)
      | `Version      ver -> Info (ver ^ "\n")
      | `Done             -> Ok
      | `Conflict (o1,o2) ->
           let msg = Printf.sprintf "Choose either %s or %s." o1 o2
           in Error (red msg)

    (* Re-exporting *)

    type token = Token.t

    (* Instantiation of the client lexer *)

    module Client = Lexer.Make (Options) (Token)

    (* Instantiation of the final lexer *)

    module Scan = LexerLib.API.Make (Config) (Client)

    (* Errors *)

    type 'item error = {
      preprocessed : string option;
      used_items   : 'item list;
      message      : string Region.reg
    }

    let format_error msg : string =
      let Region.{value; region} = msg in
      let header = region#to_string
                     ~file:(Options.input <> None)
                     ~offsets:Options.offsets
                     `Byte
      in Printf.sprintf "%s:\n%s" header value

    (* Making the preprocessor *)

    module Preproc = Preprocessor.PreprocMainGen.Make (PreprocParams)

    (* Lexical units *)

    type lex_unit = token Unit.t

    type units = lex_unit list

    (* The pipeline of self-passes on the lexical units *)

    module Pipeline = Pipeline.Make (Options) (Token) (Passes)

    (* Scanning all tokens in the input given by the CLI. *)

    let scan_all () : Std.t * (units, lex_unit error) result =
      let file =
        match Options.input with
          Some file when file <> "" -> file
        | Some _ | None -> "" in
      if Options.preprocess then
        let std, api_result = Preproc.preprocess () in
        match api_result with
          Stdlib.Error (preprocessed, message) ->
            let error = {preprocessed; used_items=[]; message}
            in std, Stdlib.Error error
        | Ok (preprocessed, _deps) ->
            (* Note: Module dependencies [_deps] are dropped. TODO *)
           let lexbuf = Lexing.from_string preprocessed in
            match Scan.from_lexbuf ~file lexbuf with
              Stdlib.Error {used_units; message} ->
                let preprocessed = Some preprocessed
                and used_items   = used_units in
                let error        = {preprocessed; used_items; message}
                in std, Stdlib.Error error
            | Ok units ->
               (* Applying the pipeline of self-passes *)
               match Pipeline.filter units with
                 Ok units -> std, Ok units
               | Error {used_units; message} ->
                   let err =
                    {preprocessed=None; used_items=used_units; message}
                   in std, Error err
      else
        let result =
          if file = "" then Scan.from_channel ~file stdin
          else Scan.from_file file in
        match result with
          Stdlib.Error {used_units; message} ->
            let preprocessed = None
            and used_items   = used_units
            and std          = Std.add_err (format_error message) Std.empty in
            let error        = {preprocessed; used_items; message}
            in std, Stdlib.Error error
        | Ok units -> Std.empty, Ok units

    (* On the one hand, parsers generated by Menhir are functions that
       expect a parameter of type [Lexing.lexbuf -> token]. On the
       other hand, we want to enable self-passes on the tokens (See
       module [Pipeline]), which implies that we scan the whole
       input before parsing. Therefore, we make believe to the
       Menhir-generated parser that we scan tokens one by one, when,
       in fact, we have lexed them all already.

       We need to use global references to store information as a way
       to workaround the signature of the parser generated by Menhir.

         * The global reference [called] tells us whether the lexer
           [scan] has been called before or not. If not, this triggers
           the scanning of all the tokens; if so, a token is extracted
           from the global reference [tokens] and the window is
           updated. The reference [called] is reset by calling
           [clear]. This is useful when the process running the
           compiler scans multiple inputs sequentially.

         * The global reference [used_tokens] holds all the tokens
           from a given source. The function [scan] updates it the
           first time it is called (see [called] above).

       In particular, when running the parsers twice, we have to call
       [clear] to reset the global state to force the scanning of all
       the tokens from the new lexing buffer.

       WARNING: By design, the state *is* observable from the
       interface of this module when calling [get_tokens], [clear] and
       [check_cli], which are exported in the signature. *)

    (* All tokens scanned up to a parse error: for debugging *)

    let used_tokens : token list ref = ref []

    (* Whether the lexer has been called or not *)

    let called : bool ref = ref false

    (* Resetting the lexer *)

    let clear () = (used_tokens := []; called := false)

    (* Filtering out the markup *)

    let filter_tokens units : token list =
      let apply tokens = function
        `Token token -> token :: tokens
      | `Markup _    -> tokens
      | `Directive d -> Token.mk_directive d :: tokens
      in List.fold_left apply [] units |> List.rev

    (* Scanning tokens one by one, based on [scan_all]. *)

    let rec scan_token =
      let store : token list ref = ref [] in
      fun lexbuf ->
        if !called then
          let token =
            match !store with
              token::tokens ->
                used_tokens := token :: !used_tokens;
                store := tokens;
                token
            | [] -> Token.mk_eof Region.ghost
          in Stdlib.Ok token
        else
          (* We drop standard output/error *)
          let _std, result = scan_all () in
          match result with
            Stdlib.Ok units ->
              store  := filter_tokens units;
              called := true;
              scan_token lexbuf
          | Error err ->
              let used_items = filter_tokens err.used_items
              in Error {err with used_items}
  end
